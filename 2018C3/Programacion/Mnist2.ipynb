{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Digit Recognition\n",
    "## With MNIST Dataset\n",
    "\n",
    "#### Firewall Windows Defender has been disabled\n",
    "\n",
    "The original MNIST Database: http://yann.lecun.com/exdb/mnist/\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the MNIST Dataset\n",
    "\n",
    "This part of the code is taken from: https://stackoverflow.com/questions/43149272/cannot-get-mnist-database-through-anaconda-jupyter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "\n",
    "def download(filename, source='http://yann.lecun.com/exdb/mnist/'):\n",
    "    print(\"Downloading %s\" % filename)\n",
    "    urlretrieve(source + filename, filename)\n",
    "\n",
    "# We then define functions for loading MNIST images and labels.\n",
    "# For convenience, they also download the requested files if needed.\n",
    "import gzip\n",
    "\n",
    "def load_mnist_images(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        download(filename)\n",
    "    # Read the inputs in Yann LeCun's binary format.\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=16)\n",
    "    # The inputs are vectors now, we reshape them to monochrome 2D images,\n",
    "    # following the shape convention: (examples, channels, rows, columns)\n",
    "    data = data.reshape(-1, 1, 28, 28)\n",
    "    # The inputs come as bytes, we convert them to float32 in range [0,1].\n",
    "    # (Actually to range [0, 255/256], for compatibility to the version\n",
    "    # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n",
    "    return data / np.float32(256)\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    if not os.path.exists(filename):\n",
    "        download(filename)\n",
    "    # Read the labels in Yann LeCun's binary format.\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        data = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    # The labels are vectors of integers now, that's exactly what we want.\n",
    "    return data\n",
    "\n",
    "\n",
    "X_train = load_mnist_images('train-images-idx3-ubyte.gz')\n",
    "y_train = load_mnist_labels('train-labels-idx1-ubyte.gz')\n",
    "X_test = load_mnist_images('t10k-images-idx3-ubyte.gz')\n",
    "y_test = load_mnist_labels('t10k-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "some_digit = X_train[36000]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[36000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4 3 5 3 6 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5) # True for all 5s, False for all other digits.\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Antonio Vega\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', max_iter=None, n_iter=None,\n",
       "       n_jobs=1, penalty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "X_tt = X_train[:]\n",
    "X_tt = X_tt.reshape(60000,784)\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_tt, y_train_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_clf.predict([X_tt[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2909a0b9a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "some_digit = X_tt[0]\n",
    "some_digit_image = some_digit.reshape(28, 28)\n",
    "\n",
    "plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
